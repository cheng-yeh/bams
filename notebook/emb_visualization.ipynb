{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6499ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from bams.data import KeypointsDataset\n",
    "from bams.models import BAMS\n",
    "from bams import HoALoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029704a7-f7af-40ed-96c7-a9eb0a625785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized for Alice dataset\n",
    "def load_data(path, f1, f2):\n",
    "    segment = 60 # in seconds\n",
    "    fz = 500\n",
    "    sample_period = int(f1.split(\".\")[0].split(\"samp\")[-1])\n",
    "    step = fz // sample_period * 10 # 10 second as a step\n",
    "    \n",
    "    # load raw train data (with annotations for 2 tasks)\n",
    "    data_train = np.load(\n",
    "        os.path.join(path, f1), allow_pickle=True\n",
    "    )\n",
    "    data_submission = np.load(\n",
    "        os.path.join(path, f2), allow_pickle=True\n",
    "    )\n",
    "\n",
    "    print(\"Subject ids in training data: \", data_train.keys())\n",
    "    print(\"Subject ids in submission data: \", data_submission.keys())\n",
    "\n",
    "    train_values = list(data_train.values())\n",
    "    submission_values = list(data_submission.values())\n",
    "    all_values = train_values + submission_values\n",
    "\n",
    "    min_len = min(map(lambda x: x.shape[0], all_values))\n",
    "    print(\"Minimum sequence length: \", min_len)\n",
    "\n",
    "    total_sample = segment * sample_period\n",
    "\n",
    "    keypoints_train = np.array([[data[start * step : start * step + total_sample] \n",
    "                                 for start in range((min_len - total_sample) // step)] \n",
    "                                for data in train_values])\n",
    "    keypoints_submission = np.array([[data[start * step : start * step + total_sample] \n",
    "                                      for start in range((min_len - total_sample) // step)] \n",
    "                                     for data in submission_values])\n",
    "    num_subject_train, num_sequence, sequence_len, num_channel = keypoints_train.shape\n",
    "    num_subject_submission, _, _, _ = keypoints_submission.shape\n",
    "    keypoints_train = keypoints_train.reshape((-1, sequence_len, num_channel))\n",
    "    keypoints_submission = keypoints_submission.reshape((-1, sequence_len, num_channel))\n",
    "    keypoints = np.concatenate([keypoints_train, keypoints_submission], axis=0)\n",
    "    \n",
    "    split_mask = np.ones(len(keypoints), dtype=bool)\n",
    "    split_mask[-num_subject_submission*num_sequence:] = False\n",
    "\n",
    "    print(\"Shape of keypoints: \", keypoints.shape)\n",
    "    print(\"Shape of split mask: \", split_mask.shape)\n",
    "\n",
    "\n",
    "\n",
    "    return keypoints, split_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2cf7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Subject ids in training data:  dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25])\n",
      "Subject ids in submission data:  dict_keys([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "Minimum sequence length:  18167\n",
      "Shape of keypoints:  (3283, 1200, 24)\n",
      "Shape, of split mask:  (3283,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 3283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garyfeederchen/Documents/PhD GNAN/Fall 2023-Summer 2024/ICML Workshop 2024/bams/bams_env/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/Users/garyfeederchen/Documents/PhD GNAN/Fall 2023-Summer 2024/ICML Workshop 2024/bams/bams_env/lib/python3.12/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:57\u001b[0m, in \u001b[0;36mQueue.__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         register_after_fork(\u001b[38;5;28mself\u001b[39m, Queue\u001b[38;5;241m.\u001b[39m_after_fork)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     58\u001b[0m     context\u001b[38;5;241m.\u001b[39massert_spawning(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# compute representations\u001b[39;00m\n\u001b[1;32m     63\u001b[0m short_term_emb, long_term_emb \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 65\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, N, L)\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PhD GNAN/Fall 2023-Summer 2024/ICML Workshop 2024/bams/bams_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PhD GNAN/Fall 2023-Summer 2024/ICML Workshop 2024/bams/bams_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PhD GNAN/Fall 2023-Summer 2024/ICML Workshop 2024/bams/bams_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:49\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     47\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mset_spawning_popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m parent_r \u001b[38;5;241m=\u001b[39m child_w \u001b[38;5;241m=\u001b[39m child_r \u001b[38;5;241m=\u001b[39m parent_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:369\u001b[0m, in \u001b[0;36mset_spawning_popen\u001b[0;34m(popen)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_spawning_popen\u001b[39m():\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(_tls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawning_popen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_spawning_popen\u001b[39m(popen):\n\u001b[1;32m    370\u001b[0m     _tls\u001b[38;5;241m.\u001b[39mspawning_popen \u001b[38;5;241m=\u001b[39m popen\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_spawning\u001b[39m(obj):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_train = \"train_24chans_fmin10_fmax25000_rwin40_samp20.pkl\"\n",
    "input_submission = \"test_24chans_fmin10_fmax25000_rwin40_samp20.pkl\"\n",
    "data_root = \"../data/alice\"\n",
    "cache_path = \"../data/alice/custom_dataset\"\n",
    "hoa_bins = 32\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "epochs = 500\n",
    "lr = 1e-3\n",
    "weight_decay = 4e-5\n",
    "log_every_step = 50\n",
    "ckpt_path = \"../bams-custom-2024-03-21-13-57-16.pt\"\n",
    "job = \"compute_representations\" # or \"train\"\n",
    "\n",
    "#if job == \"compute_representations\":\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# dataset\n",
    "if not KeypointsDataset.cache_is_available(cache_path, hoa_bins):\n",
    "    print(\"Processing data...\")\n",
    "    keypoints, split_mask = load_data(data_root, input_train, input_submission)\n",
    "else:\n",
    "    print(\"No need to process data\")\n",
    "\n",
    "# only use\n",
    "\n",
    "dataset = KeypointsDataset(\n",
    "    keypoints=keypoints,\n",
    "    hoa_bins=hoa_bins,\n",
    "    cache_path=cache_path,\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "print(\"Number of sequences:\", len(dataset))\n",
    "\n",
    "# build model\n",
    "model = BAMS(\n",
    "    input_size=dataset.input_size,\n",
    "    short_term=dict(num_channels=(64, 64, 64, 64), kernel_size=3),\n",
    "    long_term=dict(num_channels=(64, 64, 64, 64, 64), kernel_size=3, dilation=4),\n",
    "    predictor=dict(\n",
    "        hidden_layers=(-1, 256, 512, 512, dataset.target_size * hoa_bins)\n",
    "    ),\n",
    ").to(device)\n",
    "\n",
    "if ckpt_path is None:\n",
    "    raise ValueError(\"Please specify a checkpoint path\")\n",
    "\n",
    "# load checkpoint\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# compute representations\n",
    "short_term_emb, long_term_emb = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185ac14-8420-48cd-9550-5c7d87b47193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    input = data[\"input\"].float().to(device)  # (B, N, L)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        embs, hoa_pred, byol_pred = model(input)\n",
    "\n",
    "        print(\"Output: \")\n",
    "        print(embs.shape)\n",
    "        print(hoa_pred.shape)\n",
    "        print(byol_pred.shape)\n",
    "\n",
    "        short_term_emb.append(embs[\"short_term\"].detach().cpu())\n",
    "        long_term_emb.append(embs[\"long_term\"].detach().cpu())\n",
    "\n",
    "short_term_emb = torch.cat(short_term_emb)\n",
    "long_term_emb = torch.cat(long_term_emb)\n",
    "\n",
    "embs = torch.cat([short_term_emb, long_term_emb], dim=2)\n",
    "\n",
    "# embs: (B, L, N)\n",
    "batch_size, seq_len, num_feats = embs.size()\n",
    "\n",
    "embs_mean = embs.mean(1)\n",
    "embs_max = embs.max(1).values\n",
    "embs_min = embs.min(1).values\n",
    "\n",
    "embs = torch.cat([embs_mean, embs_max - embs_min], dim=-1)\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = embs.mean(0, keepdim=True), embs.std(0, unbiased=False, keepdim=True)\n",
    "embs = (embs - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774b76b-ef26-4820-ae33-5e03641da328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
